---
title: 神经网络
date: 2018-10-31 11:01:48
tags:
  - Machine Learning
  - CS231n
category: CS231n
description: 神经网络算法跟之前的线性网络计算方法不同，它的计算公式是$s = W_2\max(0, W_1x)$。其中$W_1$的含义是将图像转化成一个100维的过渡向量。非线性函数$\max(0, -)$作用于每个函数，就是简单的设置阈值。最终，矩阵$W_2$将上述的中间变量转化成10个分类的评分。分线性函数在计算上直观重要，如果省去这个函数，那么两个矩阵就能合二为一，对于评分的计算就又变成了线性函数。这个非线性函数就是**改变**的关键点。参数$W_1, W_2$通过随机梯度下降来学习到，利用反向传播通过链式法则来求导计算得到。
---

## 单神经元网络
神经网络算法领域最初是被对生物神经系统建模这一目标启发，但随后与其分道扬镳称为一个工程问题，并在机器学习领域取得很好的效果。然而，讨论将还是从对生物系统的一个高层次的简略描述开始，毕竟神经网络是从这里得到启发并发展的。

### 生物动机与连接
大脑的基本组成单元是**神经元**。人类神经系统拥有大量的神经元且彼此间通过**突触**进行连接。这里不过多的进行介绍。根据神经元结果，构建了神经元的计算模型，如下图所示：
{% asset_img neuron_model.jpg %}
在上图中，沿着轴突传播的信号($x_0$)将基于突触的强度($w_0$)与其他神经元的树突进行乘法交互($w_0x_0$)。其观点是，突触的强度($w$)是可以学习的并且可以控制一个神经元对另一个神经元的影响强度以及方向。在基本模型中，输入将信号传递到胞体内，信号在胞体内进行相加。如果最终之和高于某个阈值，那么神经元将会激活，向其轴突输出一个峰值信号。在计算模型中，我们假设峰值信号的准确时间点不重要，是激活信号频率在交流信息。基于这个速率的编码观点，将神经元的激活率建模成**激活函数**，表示轴突上激活信号的频率。激活函数通常使用**sigmoid函数$\sigma$**，该函数输入实数值，然后将输入值压缩到0-1之间。
下面给出一个神经元前向传播的实例：
``` python
class Neuron(object):
    # ...
    def forward(inputs):
        """ assume inputs and weights are 1-D numpy arrays and bias is a number. """
        cell_body_sum = np.sum(inputs * self.weights) + self.bias
        firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum))    # sigmoid activation function
        return firing_rate
```
每个神经元都是对它的输入和权重进行点积，然后加上偏差，最后使用非线性函数(激活函数)。上面程序中使用了sigmoid函数$\sigma(x) = ^1/_{(1 + e^{-x})}$。
**粗糙模型**：要注意这个对于生物神经元的建模是非常粗糙的。在实际中，有很多不同类型的神经元，每种都有不同的属性。生物神经元的树突可以进行复杂的非线性计算。突触并不就是一个简单的权重，它们是复杂的非线性动态系统。很多系统中，输出的峰值信号的精确时间点非常重要，说明速率编码的近似是不够全面的。鉴于所有这些已经介绍和更多未介绍的简化，如果你画出人类大脑和神经网络之间的类比，有神经科学背景的人对你的板书起哄也是非常自然的。

### 单神经元作为线性分类
但神经网络的前向计算的数学公式可能看起来很熟悉，就像线性网络中一样，神经元有能力对线性区域表现出倾向(激活函数接近1)或者不倾向(激活函数接近0)的能力。我们通过计算神经元输出的损失函数，可以将其变成一个线性分类器：
1.  **二分类Softmax分类器**，我们可以将$\sigma (\sum_i{w_ix_i} + b)$看成是一个分类的概率$P(y_i=1|x_i;w)$。其他分类的可能性为$P(y_i = 0|x_i;w) = 1 - P(y_i = 1|x_i; w)$，因为总概率要满足和为1。根据上面的解释，我们可以使用交叉熵的损失将其最优化为二分类的Softmax分类器(也就是逻辑回归)。因为Sigmoid函数将输出限定在0-1之间，所以分类器做出的预测的基准是神经元的输出是否带0.5。
2.  **二分类SVM分类器**，在神经元的输出增加一个最大边界折页损失(max-margin hinge loss)函数，那么就变成了二分类的SVM分类器。
**对正则化的理解**，在SVM/Softmax的例子中，正则化损失从生物角度可以看成是逐渐遗忘，因为它的效果是让所有的突触权重$w$在参数更新过程中逐渐向着0变化。
>一个单独的神经元可以用来实现一个二分类的分类器，但是无法实现非线性的网络。

## 激活函数

### Sigmoid
{% asset_img sigmoid.jpg %}
$$ \sigma(x) = \frac{1}{1 + e^{-x}} $$
将输入的实数值“挤压”到0-1的范围内。更确切的说，极小的负数变成0，极大的正数变成1。sigmoid函数在以前是非常常用的，这是因为它对神经元的激活频率有良好的解释：从完全不激活(0)到求和后最大频率处的完全激活(1)。但是现在很少使用sigmoid函数是一下两个缺点导致：
1.  **Sigmoid函数饱和使梯度消失**：当神经元的激活在接近0或者1的时候会饱和，梯度几乎会变成0。如果梯度过小，那么反向传播到数据就几乎可以忽略不记了，最终导致整个网络几乎不学习。
2.  **Sigmoid函数的输出不是零中心的**：如果输入神经元的数据总是正数，那么关于$w$的梯度在反向传播的过程中，将会要么全部是正数，要么全部是负数。这将会导致梯度下降权重更新时出现`z字形`下降，导致学习缓慢。
### Tanh
{% asset_img tanh.jpg %}
tanh将实数值压缩到[-1, 1]之间，和sigmoid函数一样**存在饱和问题**，但是和sigmoid不同的是，它的输出是零中心的。因此在实际使用中，tanh更受欢迎。同时，tanh神经元可看作是简单放大的sigmoid神经元：$tanh(x) = 2\sigma (2x) - 1$。
### ReLU
{% asset_img relu.jpg %}
$$ f(x) = \max(0, x) $$
relu是一个关于0阈值的函数，有以下特点：
1.  相较于sigmoid和tanh函数，ReLU对于随机梯度下降的收敛有着巨大的加速作用，这是由它的线性和非饱和导致的。[参考](http://www.cs.toronto.edu/~fritz/absps/imagenet.pdf)。
2.  sigmoid和tanh神经元含有指数运算等耗费计算资源的操作，而ReLU仅仅通过对矩阵进行简单的阈值计算即可得到。
3.  在训练的时候，ReLU单元比较脆弱并且可能‘死掉’。当一个很大的梯度流过ReLU的神经元的时候，可能会导致梯度更新到一种特别的状态，这种状态下神经元将无法被其他任何数据点再次激活。也就是说，ReLU单元在训练中将不可逆转的死亡，将会导致数据多样化的丢失。（学习率设置的过高，40%的神经元都会死掉，因此可以降低学习率来降低发生的概率）。
### Leaky ReLU
{% asset_img alexplot.jpg %}
$$ f(x) = \mathbb{1}(x < 0)(\alpha x) + \mathbb{1}(x >= 0)(x) $$
Leaky ReLU是为了解决ReLU死亡的问题。正如上式中，Leaky ReLU中当$x < 0$时，会给出一个很小的梯度值。这个公式性能不错，但是不是很稳定。2015年何凯明在论文中介绍了一个新的方法[PReLU](http://arxiv.org/abs/1502.01852)，将负区间上的斜率当作每个神经元中的一个参数，但是并没有证明能够适用于所有的任务。
### Maxout
Maxout是对ReLU和leaky ReLU的一般化归纳，公式为：$\max(w_1^Tx + b_1, w_2^Tx + b_2)$。ReLU和Leaky ReLU都是这个公式的特殊情况。这样，Maxout就能具有ReLU的所有优点，而没有其缺点。但是它的每个神经元的参数数量增加了一倍，导致整体参数的数量激增。

> 使用ReLU函数，要注意设置好学习率或者是监控网络中单元的死亡率。如果比较困扰，那么就直接使用Leaky Relu或者是Maxout。不要继续使用sigmoid了，可以尝试一下tanh，但是效果并不如Relu/Maxout好。

## 神经网络结构

### 灵活的组织
**将神经网络作为神经元使用图片进行展示**：神经网络可以看成是一系列的神经元组成的，神经元之间使用无环图的方式进行连接。也就是说一部分神经元的输出是另一部分神经元的输入。在网络中不允许出现循环，以防止在前向传播的时候出现无限循环的情况。不像真正的神经元，神经网络中的神经元经常是分层的。普通的神经网络中最常用的是全连接层，用来对前后两层的神经元的成对连接，而在全连接层内部的神经元之间没有任何连接。下面两个图便是使用全连接层的神经网络模型：
{% raw %}
<div class="fig figcenter fighighlight">
  <img src="/CS321n/neural-networks/neural_net.jpg" width="40%" style="display: inline-block"/>
  <img src="/CS321n/neural-networks/neural_net2.jpg" width="55%" style="display: inline-block; border-left: 1px solid black;" />
</div>
{% endraw %}


1.  **命名规则**：对于N层神经网络时，并不会将输入层计入在内。因此单层神经网络没有隐藏层，输入直接映射到输出上。在这种命名规则上，人们通常成逻辑回归或SVM为简单的单层神经网络的特例。也有人会用*Artificial Neural Networs*(ANN)或者是*Multi-Layer Perceptrons*(MLP)来替代神经网络。
2.  **输出层**：不像神经网络中的其他层，输出层通常没有激活函数(或者你认为输出层有一个线性相等的激活函数)。这是因为输出层大多用于表示评分，因此时任意值的实数，或者某种实数值的目标数。
3.  **确定网络尺寸**：用来衡量神经网络尺寸的标准主要有两个：一个是神经元的个数，另一个是参数的个数。例如：上图的第一个网络有4 + 2 = 6个神经元，[3 x 4] + [4 x 2] = 20个权重，还有4 + 2 = 6个偏置，共26个学习参数。第二个网络有4 + 4 + 1 = 9个神经元，[3 x 4] + [4 x 4] + [4 x 1] = 32个权重，4 + 4 + 1 = 9个偏置，共41个可学习的参数。

### 前向传播计算
将神经网络进行分层的主要原因是能够简单高效的使用矩阵向量操作。用上面图片中的三层神经网络为例子，输入为[3 x 1]的向量。所有处于同一层的权重可以储存在同一个矩阵中，例如第一个隐藏层的权重`$W_1$`的大小为[4 x 3]，偏置储存在向量`$b_1$`中，大小为[4 x 1]。每个单独的神经元的权重时`$W_1$`的一行，因此矩阵乘法`np.dot(W1, x)`就能计算该层中所有神经元的激活数据。类似的，`$W_2$`储存这第二个隐藏层的权重，为[4 x 4]的矩阵。`$W_3$`为[1 x 4]的矩阵，作为输出层。完整的3层的神经网络只需要经过3次矩阵的乘法即可，代码如下：
``` python
# formard-pass of a 3-layer neural network
f = lambda x: 1.0 / (1.0 + np.exp(-x))  # activation function(sigmoid).
x = np.random.randn(3, 1)               # random input vector of three numbers
h1 = f(np.dot(W1, x) + b1)
h2 = f(np.dot(W2, h1) + b2)
out = np.dot(W3, h2) + b3
```
在上面的代码中，W1，W2，W3，b1，b2，b3都是网络中可以学习的参数。注意x并不是一个单独的列向量，而可以是一个批量的训练数据（其中每个输入样本将会是x中的一列），所有的样本将会被并行化的高效计算出来。注意神经网络最后一层通常是没有激活函数的（例如，在分类任务中它给出一个实数值的分类评分）。
>全连接层的前向传播一般就是先进行一个矩阵乘法，然后加上偏置并运用激活函数。

### 表达能力
理解具有全连接层的神经网络的一个方式是，可以认为它们定义了一个由一系列函数组成的函数蔟，网络的权重就是每个函数的参数。那么该函数蔟的表达能力如何？存在不能被神经网络表达的函数吗？
现在看来，拥有至少一个隐藏层的神经网络是一个通用的近似器。给出任意联系的函数$f(x)$和任意$epsilon > 0$，存在拥有一个隐藏层的神经网络$g(x)$使得$\forall x, \mid f(x) - g(x) \mid < \epsilon$。也就是说神经网络可以近似任何连续函数。
既然一个隐藏层就能近似任意函数，那么为什么还要构建深层的网络呢？答案是虽然一个两层的网络在数学理论上能完美的近似任何连续函数，但实际运用中效果相对较差。在一个维度上，虽然以$a, b, c$为参数向量的和函数$g(x) = \sum_i c_i \mathbb{1}(a_i < x < b_i)$也是通用的近似器，但是谁也不会建议在机器学习中使用这个函数公式。神经网络在实践中非常好用，是因为他们不仅表达出的函数不仅平滑，而且对于数据的统计特性有很好的拟合。同时，神经网络通过最优化算法能比较容易地学习到这个函数。类似的，虽然在理论上深层网络和单层网络的表达能力是一样的，但就实际运用来说，深层网络效果比单层网络好。

### 设置层的数量和尺寸
在面对一个具体问题时该如何确定网络结构呢，该使用多少隐藏层，每个层的尺寸为多大？
首先，要知道当我们增加层的数量和尺寸时，网络的容量上升了。即神经元合作表达出许多复杂的函数，所以表达函数的空间得以增加。例如我们使用不同的神经网络进行训练，每个网络只有一个隐藏层，但是每个隐藏层的神经元数目不同，效果如下图：
{% asset_img layer_sizes.jpg %}
在上图中，可以看到更多的神经元的神经网络可以表达出更复杂的函数，然而这既是优势也是不足。优势是**可以分类更复杂的数据**，不足时**可能造成对训练数据的过拟合**。过拟合是网络对数据中的噪声有很强的拟合能力，而没有重视数据间的潜在的基本关系。相反，有3个隐藏层的神经元模型的表达能力只能用比较宽泛的方式去分类数据，这样能获得更好的**泛化**能力。

不要减少网络神经元数目的主要原因在于小网络更难使用梯度下降等局部方法来进行训练：虽然小型网络的损失函数的局部极小值更少，也比较容易收敛到这些局部极小值，但是这些最小值一般都很差，损失值很高。相反，大网络拥有更多的局部极小值，但就实际损失值来看，这些局部极小值表现更好，损失更小。因为神经网络是非凸的，就很难从数学上研究这些特性。在实际中，你将发现如果训练的是一个小网络，那么最终的损失值将展现出多变性：某些情况下运气好会收敛到一个好的地方，某些情况下就收敛到一个不好的极值。从另一方面来说，如果你训练一个大的网络，你将发现许多不同的解决方法，但是最终损失值的差异将会小很多。这就是说，所有的解决办法都差不多，而且对于随机初始化参数好坏的依赖也会小很多。
重申一下，正则化强度是控制神经网络过拟合的好方法。看下图结果：
{% asset_img reg_strengths.jpg %}
> 不同正则化强度的效果：每个神经网络都有20个隐层神经元，但是随着正则化强度增加，它的决策边界变得更加平滑。

*需要记住的是：不应该因为害怕出现过拟合而使用小网络。相反，应该进尽可能使用大网络，然后使用正则化技巧来控制过拟合。*